import org.apache.spark.sql.streaming.{OutputMode, Trigger}
import scala.concurrent.duration._
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark._
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.storage._
import org.apache.spark.streaming.{StreamingContext, Seconds, Minutes, Time}
import org.apache.spark.streaming.dstream._
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2
import org.apache.spark.sql.hive.thriftserver._
import org.apache.kafka.clients.consumer.ConsumerRecord 
import kafka.serializer.StringDecoder
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
//val sc = new SparkContext(conf)


val streamingContext = new StreamingContext(sc, Seconds(10))
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
val hiveSqlContext = new HiveContext(sc)
hiveSqlContext.setConf("hive.server2.thrift.port", "10007")
hiveSqlContext.setConf("hive.server2.authentication","NOSASL")
HiveThriftServer2.startWithContext(hiveSqlContext)

val file_df = sc.textFile("rdd_of_data-1532019240000").toDF
file_df.registerTempTable("rdd_of_data")
file_df.write.saveAsTable("test_database.data_test_table")

// Need to run the query to actually see it in Thrift Server
sqlContext.sql("select * from test_database.data_test_table")
hiveSqlContext.sql("select * from test_database.data_test_table")

//file.saveAsTable()